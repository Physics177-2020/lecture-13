{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0QQeAio2ChzO"
   },
   "source": [
    "# Lecture 13: Markov chain Monte Carlo\n",
    "\n",
    "[Markov chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) methods, often abbreviated as MCMC or simply MC, allow us to sample from arbitrary probability distributions. \n",
    "\n",
    "The name has two parts: a [Markov chain](https://en.wikipedia.org/wiki/Markov_chain) is a stochastic model that undergoes transitions between different configurations, with the key condition that the probability of any event/transition depends only on the current configuration of the system. And, as you've seen before, [Monte Carlo](https://en.wikipedia.org/wiki/Monte_Carlo_method) methods are ones that make use of random sampling.\n",
    "\n",
    "MCMC works by constructing a stochastic model -- a Markov chain -- whose stationary distribution is the same as the one that we want to sample from. Monte Carlo simulations of the Markov chain then provide us with samples from the desired distribution.\n",
    "\n",
    "The Markov chain is defined by a set of transition probabilities $T_{ij}$ between configurations labeled $i$ and $j$. We can ensure that the stationary distribution of the Markov chain is the same as the one that we want to sample by enforcing the [detailed balance](https://en.wikipedia.org/wiki/Detailed_balance) condition\n",
    "\n",
    "$$\n",
    "P(i)T_{ij} = P(j)T_{ji}\\,,\n",
    "$$\n",
    "\n",
    "where $P(i)$ is the probability of configuration $i$. Usually, we aren't able to compute the $P(i)$ exactly, but we can often find their ratios:\n",
    "\n",
    "$$\n",
    "\\frac{P(i)}{P(j)} = \\frac{T_{ji}}{T_{ij}}\\,.\n",
    "$$\n",
    "\n",
    "If $P$ is a Gibbs distribution, then this condition is\n",
    "\n",
    "$$\n",
    "\\frac{T_{ij}}{T_{ji}} = \\frac{P(j)}{P(i)} = e^{\\beta \\left(E(i) - E(j)\\right)}\\,.\n",
    "$$\n",
    "\n",
    "Note that there are **many possible ways** to choose the $T_{ij}$ so that the detailed balance condition is satisfied! One common choice is called the [Metropolis rule](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm),\n",
    "\n",
    "$$\n",
    "T_{ij} = \\begin{cases} \n",
    "1 \\qquad\\qquad\\,\\, \\text{if }E(j) < E(i)\\\\\n",
    "e^{-\\beta\\left(E(j) - E(i)\\right)} \\;\\; \\text{if }E(j) > E(i)\n",
    "\\end{cases} \\,.\n",
    "$$\n",
    "\n",
    "\n",
    "### Example: MCMC sampling for a single spin in an external magnetic field\n",
    "\n",
    "Let's return to the example of a single spin in an external magnetic field, and let's use MCMC to compute its probability distribution. This is an easy case where we can check our answer analytically. First, let's confirm the analytical result when the interaction energy $\\epsilon = 1$ and the temperature $T=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "KzKZdkgIChzQ",
    "outputId": "05d2c11e-f033-476c-f9e3-b2c5245d0f88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At eps = 1, T = 1, the probability of spin up is 0.880797 and spin down is 0.119203\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Taking our distribution from previous lectures\n",
    "\n",
    "def gibbs(eps, T):\n",
    "    \"\"\" This function takes the energy eps and temperature T as input\n",
    "        and returns the Gibbs distribution for a single spin as output \"\"\"\n",
    "    \n",
    "    Z     = np.exp(-eps/T) + np.exp(eps/T)\n",
    "    p_pos = np.exp( eps/T) / Z\n",
    "    p_neg = np.exp(-eps/T) / Z\n",
    "    \n",
    "    return p_pos, p_neg\n",
    "\n",
    "\n",
    "# Evaluate the distribution at eps = 1, T = 1\n",
    "\n",
    "p_pos, p_neg = gibbs(1, 1)\n",
    "\n",
    "print('At eps = 1, T = 1, the probability of spin up is %lf and spin down is %lf' % (p_pos, p_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BcOdR91uChzT"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uv4EoIhjChzX"
   },
   "source": [
    "Now we'll write a simulation to *sample* from the Gibbs distribution without computing it explicitly.\n",
    "\n",
    "The first thing to do is to write down our transition probabilities. There are only two configurations: $\\sigma = \\pm 1$. With our parameters, the energy of the $\\sigma = 1$ configuration is lower than the energy of the $\\sigma = -1$ configuration. Thus, the transition probabilities using the Metropolis rule should be\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "T_{-+} &= 1\\,, \\\\\n",
    "T_{+-} &= e^{-\\beta\\left(E(-1) - E(1)\\right)} = e^{-2\\beta\\epsilon}\\,.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To perform the Monte Carlo simulation, we'll start with an arbitrary configuration (say, $\\sigma=1$) and simulate many random transitions. We'll also store a record of the configurations as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "99JrPRIuChzX",
    "outputId": "a05e069f-a9c8-4159-d7e1-eaed17f09e92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "\n",
    "# Store the configurations\n",
    "\n",
    "configs = []\n",
    "\n",
    "# Initialize sigma and transition rates\n",
    "\n",
    "sigma = 1\n",
    "T_mp  = 1          # transition from - to + state\n",
    "T_pm  = np.exp(-2) # transition from + to - state\n",
    "\n",
    "# Choose the number of steps for our simulation, and go!\n",
    "\n",
    "n_steps = 10**3\n",
    "\n",
    "for i in range(n_steps):\n",
    "    \n",
    "    # Store the current configuration\n",
    "    configs.append(sigma)\n",
    "    \n",
    "    # Random transition\n",
    "    if sigma==-1:\n",
    "      sigma *= -1\n",
    "      \n",
    "    elif sigma==1:\n",
    "      r = rng.rand()\n",
    "      if r<T_pm:\n",
    "        sigma *= -1\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-2jGALeChzb"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EQHXSgLPChze"
   },
   "source": [
    "Now the list `configs` is storing a sample from the probability distribution! Let's check to see how accurate it is. We can compute the probability of the $\\sigma=\\pm1$ configurations by measuring how often these values appear in `configs`. \n",
    "\n",
    "**Note**: we took $10^6$ samples in our simulation. How accurate do you expect the probabilities to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "r-AsSf5HChzf",
    "outputId": "8521f847-8bd3-4daf-8c68-ba6f09ccdf7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(+1) true: 0.880797\tsample: 0.878000\n",
      "P(-1) true: 0.119203\tsample: 0.122000\n"
     ]
    }
   ],
   "source": [
    "# Convert configs into a numpy array so that we can do vector calculations\n",
    "configs = np.array(configs)\n",
    "\n",
    "# How often was sigma = 1 in the sample?\n",
    "p_pos_sample = np.sum(configs==1) / float(len(configs))\n",
    "\n",
    "# How often was sigma = -1 in the sample?\n",
    "p_neg_sample = np.sum(configs==-1) / float(len(configs))\n",
    "\n",
    "# Compare these with the analytical values\n",
    "print('P(+1) true: %lf\\tsample: %lf' % (p_pos, p_pos_sample))\n",
    "print('P(-1) true: %lf\\tsample: %lf' % (p_neg, p_neg_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ALT1G0fvZdbG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "lecture-13.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
